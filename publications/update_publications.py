#!/usr/bin/env python3
"""
update_publications.py
-------------------------------------------
Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Google Scholar
(Ruslan Afasizhev, Inna Afasizheva), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
Ğ² JSON Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ»Ğ¾Ğº <section id="publications"> â€¦ </section>
Ğ² ĞºĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ¼ index.html.
-------------------------------------------
pip install "scholarly>=1.7.11"
"""

import json
import re
from datetime import datetime
from pathlib import Path

from scholarly import scholarly

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCRIPT_DIR = Path(__file__).resolve().parent          # â€¦/publications
REPO_DIR   = SCRIPT_DIR.parent                       # â€¦/afasilab.github.io
INDEX_HTML = REPO_DIR / "index.html"

GS_DIR     = REPO_DIR / "publications" / "gs_json"
GS_DIR.mkdir(parents=True, exist_ok=True)

today        = datetime.today().strftime("%m-%d-%Y")
JSON_FILE    = GS_DIR / f"publications_detailed_{today}.json"

AUTHOR_IDS = {
    "Ruslan Afasizhev": "U7yVbHIAAAAJ",
    "Inna Afasizheva":  "-ivXdnsAAAAJ",
}
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def fetch_author_pubs(author_id: str) -> list[dict]:
    """Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾ ĞµĞ³Ğ¾ Google-Scholar ID."""
    base = scholarly.search_author_id(author_id)
    full = scholarly.fill(base, sections=["publications"])
    pubs = []
    for pub in full["publications"]:
        filled = scholarly.fill(pub)
        bib    = filled.get("bib", {})
        pubs.append(
            {
                "title":        bib.get("title", ""),
                "authors":      bib.get("author", ""),
                "year":         bib.get("pub_year", ""),
                "journal":      bib.get("journal", ""),
                "volume":       bib.get("volume", ""),
                "number":       bib.get("number", ""),
                "pages":        bib.get("pages", ""),
                "publisher":    bib.get("publisher", ""),
                "abstract":     bib.get("abstract", ""),
                "num_citations": filled.get("num_citations", 0),
                "pub_url":      filled.get("pub_url", ""),
            }
        )
    return pubs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_pubs = []
for name, aid in AUTHOR_IDS.items():
    print(f"ğŸ“¥  Fetching publications for {name} â€¦")
    all_pubs.extend(fetch_author_pubs(aid))

# Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ (title + year)
uniq: dict[tuple[str, str], dict] = {}
for p in all_pubs:
    key = (p["title"].lower().strip(), p.get("year"))
    uniq.setdefault(key, p)
publications = list(uniq.values())

# Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ±ĞµĞ· Ğ³Ğ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼
publications = [p for p in publications if p.get("year")]
publications.sort(key=lambda x: int(x["year"]), reverse=True)

print(f"ğŸ”¢  Total unique publications: {len(publications)}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with JSON_FILE.open("w", encoding="utf-8") as f:
    json.dump(publications, f, ensure_ascii=False, indent=2)
print(f"ğŸ’¾  Saved metadata â†’ {JSON_FILE.relative_to(REPO_DIR)}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ HTML Ğ±Ğ»Ğ¾Ğº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
blocks = []
for p in publications:
    cite = f'{p["authors"]}. {p["title"]}.'
    if p["journal"]:
        cite += f' <em>{p["journal"]}</em>'
    if p["volume"]:
        cite += f', <strong>{p["volume"]}</strong>'
    if p["number"]:
        cite += f'({p["number"]})'
    if p["pages"]:
        cite += f', {p["pages"]}'
    cite += f', {p["year"]}.'

    url  = p["pub_url"]
    blocks.append(
        f'''    <div class="pub-entry">
      <p>{cite}<br><em>(<a href="{url}" target="_blank">Link</a>)</em></p>
    </div>'''
    )

new_section = (
    "<section id=\"publications\">\n"
    "  <h2>Publications</h2>\n"
    "  <div class=\"publications\">\n"
    f"{chr(10).join(blocks)}\n"
    "  </div>\n"
    "</section>"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² index.html â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
html = INDEX_HTML.read_text(encoding="utf-8")

pattern = r"<section[^>]*id=[\"']publications[\"'][\s\S]*?</section>"
updated = re.sub(pattern, new_section, html, flags=re.IGNORECASE | re.DOTALL)

if updated != html:
    INDEX_HTML.write_text(updated, encoding="utf-8")
    print("âœ… index.html updated with fresh publication list")
else:
    print("â„¹ï¸  index.html already up-to-date â€” no changes made")
